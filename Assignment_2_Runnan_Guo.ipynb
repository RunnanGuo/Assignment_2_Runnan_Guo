{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2de68116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_variance(data, ddof=0):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    return sum((x - mean) ** 2 for x in data) / (n - ddof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7fd0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import graphviz as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4190b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_cov(dim):\n",
    "    acc  = []\n",
    "    for i in range(dim):\n",
    "        row = np.ones((1,dim)) * corr\n",
    "        row[0][i] = 1\n",
    "        acc.append(row)\n",
    "    return np.concatenate(acc,axis=0)\n",
    "\n",
    "\n",
    "def fn_generate_multnorm(nobs,corr,nvar):\n",
    "\n",
    "    mu = np.zeros(nvar)\n",
    "    std = (np.abs(np.random.normal(loc = 1, scale = .5,size = (nvar,1))))**(1/2)\n",
    "    # generate random normal distribution\n",
    "    acc = []\n",
    "    for i in range(nvar):\n",
    "        acc.append(np.reshape(np.random.normal(mu[i],std[i],nobs),(nobs,-1)))\n",
    "    \n",
    "    normvars = np.concatenate(acc,axis=1)\n",
    "\n",
    "    cov = fn_generate_cov(nvar)\n",
    "    C = np.linalg.cholesky(cov)\n",
    "\n",
    "    Y = np.transpose(np.dot(C,np.transpose(normvars)))\n",
    "\n",
    "#     return (Y,np.round(np.corrcoef(Y,rowvar=False),2))\n",
    "    return Y\n",
    "\n",
    "\n",
    "def fn_randomize_treatment(N,p=0.5):\n",
    "    treated = random.sample(range(N), round(N*p))\n",
    "    return np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72bd78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data(tau,N,p,p0,corr,conf = True,flagX = False):\n",
    "    nvar = p+2 # 1 confounder and variable for randomizing treatment\n",
    "    corr = 0.5 # correlation for multivariate normal\n",
    "\n",
    "    if conf==False:\n",
    "        conf_mult = 0 # remove confounder from outcome\n",
    "        \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    W0 = allX[:,0].reshape([N,1]) # variable for RDD assignment\n",
    "    C = allX[:,1].reshape([N,1]) # confounder\n",
    "    X = allX[:,2:] # observed covariates\n",
    "    \n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    beta0 = np.random.normal(5,5,[p,1])\n",
    "    \n",
    "    beta0[p0:p] = 0 # sparse model\n",
    "    Yab = tau*T+X@beta0+conf_mult*0.6*C+err\n",
    "    if flagX==False:\n",
    "        return (Yab,T)\n",
    "    else:\n",
    "        return (Yab,T,X)\n",
    "    \n",
    "    \n",
    "def fn_tauhat_means(Yt,Yc):\n",
    "    nt = len(Yt)\n",
    "    nc = len(Yc)\n",
    "    tauhat = np.mean(Yt)-np.mean(Yc)\n",
    "    se_tauhat = (np.var(Yt,ddof=1)/nt+np.var(Yc,ddof=1)/nc)**(1/2)\n",
    "    return (tauhat,se_tauhat)\n",
    "\n",
    "\n",
    "def fn_bias_rmse_size(theta0,thetahat,se_thetahat,cval = 1.96):\n",
    "    \"\"\"\n",
    "    theta0 - true parameter value\n",
    "    thetatahat - estimated parameter value\n",
    "    se_thetahat - estiamted se of thetahat\n",
    "    \"\"\"\n",
    "    b = thetahat - theta0\n",
    "    bias = np.mean(b)\n",
    "    rmse = np.sqrt(np.mean(b**2))\n",
    "    tval = b/se_thetahat # paramhat/se_paramhat H0: theta = 0\n",
    "    size = np.mean(1*(np.abs(tval)>cval))\n",
    "    # note size calculated at true parameter value\n",
    "    return (bias,rmse,size)    \n",
    "    \n",
    "    \n",
    "def fn_run_experiments(tau,Nrange,p,p0,corr,conf,flagX=False):\n",
    "    n_values = []\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    lb = []\n",
    "    ub = []\n",
    "    for N in tqdm(Nrange):\n",
    "        n_values = n_values + [N]\n",
    "        if flagX==False:\n",
    "            Yexp,T = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Yt = Yexp[np.where(T==1)[0],:]\n",
    "            Yc = Yexp[np.where(T==0)[0],:]\n",
    "            tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)            \n",
    "        elif flagX==1:\n",
    "            # use the right covariates in regression\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs = X[:,:p0]\n",
    "            covars = np.concatenate([T,Xobs],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "        elif flagX==2:\n",
    "            # use some of the right covariates and some \"wrong\" ones\n",
    "            Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "            Xobs1 = X[:,:np.int(p0/2)]\n",
    "            Xobs2 = X[:,-np.int(p0/2):]\n",
    "            covars = np.concatenate([T,Xobs1,Xobs2],axis = 1)\n",
    "            mod = sm.OLS(Yexp,covars)\n",
    "            res = mod.fit()\n",
    "            tauhat = res.params[0]\n",
    "            se_tauhat = res.HC1_se[0]\n",
    "            \n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]    \n",
    "        lb = lb + [tauhat-1.96*se_tauhat]\n",
    "        ub = ub + [tauhat+1.96*se_tauhat]\n",
    "        \n",
    "    return (n_values,tauhats,sehats,lb,ub)\n",
    "\n",
    "\n",
    "def fn_plot_with_ci(n_values,tauhats,tau,lb,ub,caption):\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    plt.plot(n_values,tauhats,label = '$\\hat{\\\\tau}$')\n",
    "    plt.xlabel('N')\n",
    "    plt.ylabel('$\\hat{\\\\tau}$')\n",
    "    plt.axhline(y=tau, color='r', linestyle='-',linewidth=1,\n",
    "                label='True $\\\\tau$={}'.format(tau))\n",
    "    plt.title('{}'.format(caption))\n",
    "    plt.fill_between(n_values, lb, ub,\n",
    "        alpha=0.5, edgecolor='#FF9848', facecolor='#FF9848',label = '95% CI')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07537d",
   "metadata": {},
   "source": [
    "# 1. Experiment with covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb415244",
   "metadata": {},
   "source": [
    "## Simulate a DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38c5de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "T = np.random.binomial(1, 0.4, 1500)\n",
    "X = np.random.normal(16,8,1500)\n",
    "Y = np.random.normal(3 + 5*T + 2*X, 3).astype(int)\n",
    "datacov = pd.DataFrame(dict(T = T,\n",
    "                            X = X,\n",
    "                            Y = Y))\n",
    "datacov.to_csv(\"datacov.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fab34",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74961fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"210pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 210.29 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-112 206.2918,-112 206.2918,4 -4,4\"/>\n",
       "<!-- treatment -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>treatment</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"44.846\" cy=\"-90\" rx=\"44.6926\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.846\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">treatment</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"99.846\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99.846\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- treatment&#45;&gt;y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>treatment&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M58.16,-72.5708C65.1144,-63.4668 73.7357,-52.1808 81.33,-42.2391\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.1195,-44.3531 87.4086,-34.2817 78.5568,-40.1038 84.1195,-44.3531\"/>\n",
       "</g>\n",
       "<!-- covariates -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>covariates</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"154.846\" cy=\"-90\" rx=\"47.3916\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.846\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">covariates</text>\n",
       "</g>\n",
       "<!-- covariates&#45;&gt;y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>covariates&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M141.5321,-72.5708C134.5776,-63.4668 125.9563,-52.1808 118.362,-42.2391\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.1352,-40.1038 112.2834,-34.2817 115.5725,-44.3531 121.1352,-40.1038\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11f93e1c0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"treatment\", \"y\")\n",
    "g.edge(\"covariates\", \"y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26717807",
   "metadata": {},
   "source": [
    "### a. You do not control for any covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bdf60622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "corr = 0.5\n",
    "conf = False\n",
    "p = 10\n",
    "p0 = 5\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65f21d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1627.00it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 182.31it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T = fn_generate_data(tau,N,10,0,corr,conf)\n",
    "        Yt = Yexp[np.where(T==1)[0],:]\n",
    "        Yc = Yexp[np.where(T==0)[0],:]\n",
    "        tauhat,se_tauhat = fn_tauhat_means(Yt,Yc)\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47154a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.00509644220867979, RMSE=0.2106475717820159, size=0.072\n",
      "N=1000: bias=0.002530775974719494, RMSE=0.062246501438701904, size=0.051\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6726744",
   "metadata": {},
   "source": [
    "### b. You control for all the covariates that affect the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3488ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [00:01<00:00, 983.00it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:06<00:00, 162.57it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "flagX = 1\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    const = const = np.ones([N,1])\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,X = fn_generate_data(tau,N,p,p0,corr,conf,flagX)\n",
    "        mod = sm.OLS(Yexp,np.concatenate([T,X,const],axis = 1))\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63a4a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.002155984201206686, RMSE=0.21026332074861676, size=0.061\n",
      "N=1000: bias=-0.0049673904906797255, RMSE=0.060919682984843876, size=0.044\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccfddf",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaa1fe",
   "metadata": {},
   "source": [
    "y: education \n",
    "\n",
    "treatment: education cash transfer to households (to encourage education)  \n",
    "\n",
    "covariates: age, gender (uncorrelated with the treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac77477",
   "metadata": {},
   "source": [
    "# 2. Experiment with a confounder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88d088",
   "metadata": {},
   "source": [
    "## Simulate a DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7055e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "X = np.random.normal(16,8,1500)\n",
    "T = np.random.binomial(1, 1/(2+X**2), 1500)\n",
    "Y = np.random.normal(3 + 5*T + 2*X, 3).astype(int)\n",
    "dataconf = pd.DataFrame(dict(T = T,\n",
    "                            X = X,\n",
    "                            Y = Y))\n",
    "dataconf.to_csv(\"dataconf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb8f4",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f68151e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"141pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 140.84 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 136.8414,-184 136.8414,4 -4,4\"/>\n",
       "<!-- treatment -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>treatment</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"44.846\" cy=\"-90\" rx=\"44.6926\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.846\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">treatment</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"80.846\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.846\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- treatment&#45;&gt;y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>treatment&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M53.7449,-72.2022C57.9482,-63.7955 63.0501,-53.5917 67.6886,-44.3149\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"70.9557,-45.6068 72.2974,-35.0972 64.6947,-42.4762 70.9557,-45.6068\"/>\n",
       "</g>\n",
       "<!-- confounder -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>confounder</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"80.846\" cy=\"-162\" rx=\"51.9908\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.846\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">confounder</text>\n",
       "</g>\n",
       "<!-- confounder&#45;&gt;treatment -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>confounder&#45;&gt;treatment</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M71.9471,-144.2022C67.8365,-135.981 62.8665,-126.041 58.3111,-116.9301\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.37,-115.2216 53.7673,-107.8425 55.109,-118.3521 61.37,-115.2216\"/>\n",
       "</g>\n",
       "<!-- confounder&#45;&gt;y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>confounder&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M88.514,-143.8756C92.4599,-133.5642 96.8515,-120.2907 98.846,-108 101.4089,-92.2066 101.4089,-87.7934 98.846,-72 97.3897,-63.0258 94.6554,-53.5277 91.7498,-45.0413\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"95.0356,-43.8355 88.3264,-35.6357 88.4577,-46.2297 95.0356,-43.8355\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11fa7cc70>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"treatment\", \"y\")\n",
    "g.edge(\"confounder\",\"treatment\")\n",
    "g.edge(\"confounder\",\"y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6a361",
   "metadata": {},
   "source": [
    "### a. You fail to control for the confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c39d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data_conf(tau,N,p,corr,conf = True):\n",
    "    nvar = p+2 # 1 confounder and variable for randomizing treatment\n",
    "    corr = 0.5 # correlation for multivariate normal\n",
    "    if conf==False:\n",
    "        conf_mult = 0\n",
    "    else:\n",
    "        conf_mult = 1 \n",
    "        \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    C = allX[:,1].reshape([N,1]) # confounder\n",
    "    X = allX[:,2:] # observed covariates\n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    beta0 = np.random.normal(5,5,[p,1])\n",
    "    \n",
    "    Yab = tau*T+X@beta0+conf_mult*0.6*C+err\n",
    "    \n",
    "    return (Yab,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0cbc05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "corr = 0.5\n",
    "p = 10\n",
    "N = 1000\n",
    "conf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4e7b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1146.72it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 172.20it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    const = const = np.ones([N,1])\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,C = fn_generate_data_conf(tau,N,p,corr)\n",
    "        mod = sm.OLS(Yexp,np.concatenate([T,const],axis = 1))\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a60fe182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.012504013725042711, RMSE=8.305823750062967, size=0.058\n",
      "N=1000: bias=0.020409954555835823, RMSE=2.541150982227619, size=0.042\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4151b5",
   "metadata": {},
   "source": [
    "### b. You do control for the confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e60a2e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1152.04it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 173.84it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    const = const = np.ones([N,1])\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,C = fn_generate_data_conf(tau,N,p,corr)\n",
    "        mod = sm.OLS(Yexp,np.concatenate([T,C,const],axis = 1))\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "842418c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.13482034317643987, RMSE=5.905420041634713, size=0.042\n",
      "N=1000: bias=0.014270608780753261, RMSE=1.8783948012049756, size=0.049\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007715e",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1177de8",
   "metadata": {},
   "source": [
    "y: employment status\n",
    "\n",
    "treatment: job training \n",
    "\n",
    "confounders: enterprising personality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5c126",
   "metadata": {},
   "source": [
    "# 3. Experiment with a selection bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90cee3a",
   "metadata": {},
   "source": [
    "## Simulate a DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b27e344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "T = np.random.binomial(1, 0.4, 1500)\n",
    "Y = np.random.normal(3 + 5*T, 3).astype(int)\n",
    "X = np.random.normal(7+np.mean(T)*np.mean(Y),3,1500)\n",
    "databias = pd.DataFrame(dict(T = T,\n",
    "                            X = X,\n",
    "                            Y = Y))\n",
    "databias.to_csv(\"databias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872eabff",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f5cbc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"107pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 106.85 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 102.846,-184 102.846,4 -4,4\"/>\n",
       "<!-- treatment -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>treatment</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"54\" cy=\"-162\" rx=\"44.6926\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">treatment</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- treatment&#45;&gt;y -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>treatment&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M47.1868,-143.8314C44.1902,-135.8406 40.6057,-126.2819 37.2943,-117.4514\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"40.506,-116.0478 33.7175,-107.9134 33.9517,-118.5057 40.506,-116.0478\"/>\n",
       "</g>\n",
       "<!-- bias -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>bias</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">bias</text>\n",
       "</g>\n",
       "<!-- treatment&#45;&gt;bias -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>treatment&#45;&gt;bias</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M57.8413,-143.6001C59.8166,-133.2025 62.0123,-119.9346 63,-108 64.3197,-92.0545 64.3197,-87.9455 63,-72 62.2945,-63.4753 60.9727,-54.2703 59.5551,-45.917\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"62.9689,-45.1258 57.7474,-35.9069 56.0803,-46.3698 62.9689,-45.1258\"/>\n",
       "</g>\n",
       "<!-- y&#45;&gt;bias -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>y&#45;&gt;bias</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M33.6742,-72.2022C36.7476,-64.0064 40.4616,-54.1024 43.8695,-45.0145\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47.1685,-46.1853 47.4026,-35.593 40.6142,-43.7274 47.1685,-46.1853\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11f9b7d90>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Digraph()\n",
    "g.edge(\"treatment\", \"y\")\n",
    "g.edge(\"treatment\",\"bias\")\n",
    "g.edge(\"y\",\"bias\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b14c9c",
   "metadata": {},
   "source": [
    "### a. You control for the variable in between the path from cause to effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14ad50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_generate_data_bias(tau,N,p,corr):\n",
    "\n",
    "    nvar = p+1 \n",
    "    corr = 0.5 \n",
    " \n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    T = fn_randomize_treatment(N) \n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    u = np.random.normal(0,1,[N,1])\n",
    "    Yab = tau*T+err\n",
    "    Bab = 0.2*T+0.6*Yab+u\n",
    "\n",
    "    return (Yab,T,Bab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0450f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "corr = .5\n",
    "p = 10\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cedd551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1182.74it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 175.87it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    const = const = np.ones([N,1])\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,B = fn_generate_data_bias(tau,N,p,corr)\n",
    "        mod = sm.OLS(Yexp,np.concatenate([T,B,const],axis = 1))\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "        \n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "433c77da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-1.412800985468613, RMSE=1.444057461064614, size=0.996\n",
      "N=1000: bias=-1.4154562528916956, RMSE=1.4186072662220746, size=1.0\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba08fe",
   "metadata": {},
   "source": [
    "### b. You do not control for the variable in between the path from cause to effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0143a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1206.12it/s]\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 172.48it/s]\n"
     ]
    }
   ],
   "source": [
    "estDict = {}\n",
    "R = 1000\n",
    "for N in [100,1000]:\n",
    "    tauhats = []\n",
    "    sehats = []\n",
    "    const = const = np.ones([N,1])\n",
    "    for r in tqdm(range(R)):\n",
    "        Yexp,T,B = fn_generate_data_bias(tau,N,p,corr)\n",
    "        mod = sm.OLS(Yexp,np.concatenate([T,const],axis = 1))\n",
    "        res = mod.fit()\n",
    "        tauhat = res.params[0]\n",
    "        se_tauhat = res.HC1_se[0]\n",
    "        tauhats = tauhats + [tauhat]\n",
    "        sehats = sehats + [se_tauhat]\n",
    "        \n",
    "    estDict[N] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b062c0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0001322326668914915, RMSE=0.1984563143185186, size=0.051\n",
      "N=1000: bias=-0.0009308912756163661, RMSE=0.06376588588676245, size=0.049\n"
     ]
    }
   ],
   "source": [
    "tau0 = tau*np.ones([R,1])\n",
    "for N, results in estDict.items():\n",
    "    (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                         results['sehat'])\n",
    "    print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a14ba8",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de4e02",
   "metadata": {},
   "source": [
    "y: weight loss\n",
    "\n",
    "treatment: online fitness tutorial\n",
    "\n",
    "selection: self-report in the comments\n",
    "\n",
    "（Those who do not have significant weight loss are likely not to report in the comment and may not participate the entire tutorial)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
